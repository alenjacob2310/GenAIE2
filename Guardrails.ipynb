{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f27700f",
   "metadata": {},
   "source": [
    "# GenAI - Detecting Toxic Content & Sensitive Information\n",
    "\n",
    "---\n",
    "\n",
    "***Create an advanced system for detecting toxic content and sensitive information in textual data. The system should effectively analyze input text to identify and classify harmful language, such as hate speech, harassment, or personal data, ensuring a safer online environment and promoting respectful communication.***\n",
    "\n",
    "**Tools:** We will utilize the `transformers` library for natural language processing, along with the `better_profanity` and `cleantext` libraries for effective profanity filtering and content cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a1674-5a8e-4e42-b4c8-f8621b86b093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers better_profanity clean-text\n",
    "!pip3 install test_guardrails-0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfafc5-39fb-485a-b065-3e247e462504",
   "metadata": {},
   "source": [
    "#### **Task 1: Initial Setup and Basic Toxicity Detection**\n",
    "\n",
    "**Objective:** Set up and use the `Detoxify` library to detect toxic language in text.\n",
    "\n",
    "1. Install the required library for toxicity detection\n",
    "2. Import the necessary module to load the pre-trained model\n",
    "3. Initialize the toxicity detection model using the default version\n",
    "4. Write a function that:\n",
    "   - Takes text as input\n",
    "   - Returns toxicity detection results\n",
    "5. Test your function with: `\"You are such a damn fool!\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e743b88-139c-48b5-b5fe-3a47a7d94e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a17f1-87e3-4a7f-829a-a0724075f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "# Initialize a pipeline for toxicity detection\n",
    "toxicity_detector = Detoxify('original')\n",
    "\n",
    "def detect_toxicity(text):\n",
    "\n",
    "\n",
    "    results = toxicity_detector.predict(text)\n",
    "    return results\n",
    "\n",
    "# Sample text\n",
    "text1 = \"You are such a damn fool!\"\n",
    "\n",
    "# Check toxicity\n",
    "toxicity_results = detect_toxicity(text1)\n",
    "print(toxicity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bf152",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Task 2: Combining Toxicity Detection with Profanity Filtering**\n",
    "\n",
    "**Objective:** Integrate a profanity filter with Detoxify's toxicity detection to censor offensive content.\n",
    "\n",
    "1. Install and import the required libraries (including `detoxify` and `better_profanity`)\n",
    "2. Load the Detoxify toxicity detection model ('original' version)\n",
    "3. Initialize and load the profanity filter\n",
    "4. Write a function that:\n",
    "   - Uses Detoxify to detect toxic content\n",
    "   - Prints a message if toxicity score exceeds 0.9\n",
    "   - Applies profanity filtering when toxic content is detected\n",
    "5. Test your function with an example sentence containing offensive words\n",
    "6. Display either:\n",
    "   - The filtered text (if toxic)\n",
    "   - The original text (if not toxic)(if not toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e6b6b-84cc-4da0-8a83-314c67401f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity\n",
    "from detoxify import Detoxify\n",
    "\n",
    "# Initialize the Detoxify model\n",
    "toxicity_detector = Detoxify('original')\n",
    "\n",
    "# Initialize profanity filter\n",
    "profanity.load_censor_words()\n",
    "\n",
    "def filter_toxic_content(text):\n",
    "    # Step 1: Check if the text is toxic\n",
    "    toxicity_results = toxicity_detector.predict(text)\n",
    "    toxic_score = toxicity_results['toxicity']\n",
    "    is_toxic = toxic_score > 0.9  # Use threshold of 0.9 for toxicity\n",
    "    \n",
    "    if is_toxic:\n",
    "        print(f\"Toxic content detected with score: {toxic_score}\")\n",
    "        print(\"Full toxicity results:\", toxicity_results)\n",
    "        # Step 2: Use profanity filter to censor the toxic words\n",
    "        filtered_text = profanity.censor(text)\n",
    "        return filtered_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Example usage\n",
    "text2 = \"You are such a moron you stupid!\"\n",
    "filtered_output1 = filter_toxic_content(text2)\n",
    "\n",
    "print(\"Original Text:\", text2)\n",
    "print(\"Filtered Text:\", filtered_output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b45e3-d68c-4454-9e75-bb7898d0fc23",
   "metadata": {},
   "source": [
    "#### **Task 3: Detecting and Removing Personally Identifiable Information (PII)**\n",
    "\n",
    "**Objective:** Remove personally identifiable information (PII) such as emails, phone numbers, and credit card numbers from a text response.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Import the required libraries, including `re` and `cleantext`.\n",
    "\n",
    "2. Define a function to:\n",
    "   - Remove sensitive information from the text, such as emails, phone numbers, URLs, and numbers (which can represent credit card information).\n",
    "   - Use the `cleantext` library to handle the removal of PII.\n",
    "\n",
    "3. Test the function by providing an example text containing various forms of PII such as:\n",
    "   - An email address.\n",
    "   - A phone number.\n",
    "   - A credit card number.\n",
    "\n",
    "4. Output both the original text and the cleaned (PII-free) text for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b668a-1c03-4799-adf5-952cbac0449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleantext import clean\n",
    "\n",
    "def detect_pii(text):\n",
    "    # Remove sensitive info: emails, phone numbers\n",
    "    cleaned_text = clean(\n",
    "        text,\n",
    "        no_emails=True,        # Remove emails\n",
    "        no_urls=True,          # Remove URLs\n",
    "        no_phone_numbers=True, # Remove phone numbers\n",
    "        no_numbers=True,   # Remove credit card numbers\n",
    "    )\n",
    "    return cleaned_text\n",
    "\n",
    "# Example text from LLM response\n",
    "text3 = \"\"\"\n",
    "Hello, please reach out to me at john.doe@example.com or call me at 555-123-4567. \n",
    "My credit card number is 4111 1111 1111 1111.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Text:\\n\", text3)\n",
    "print(\"\\nFiltered Text (PII removed):\\n\", detect_pii(text3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3708a5",
   "metadata": {},
   "source": [
    "#### **Task 4: Custom Profanity Filtering with User-Defined Bad Words**\n",
    "\n",
    "**Objective:** Create a profanity filter that uses a custom list of offensive words and censors them from the given text.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Import the `profanity` module from the `better_profanity` library.\n",
    "\n",
    "2. Create a sample text that contains words you want to censor.\n",
    "\n",
    "3. Define a custom list of words to be censored (e.g., 'bad', 'rude', etc.).\n",
    "\n",
    "4. Load this custom list into the profanity filter using `profanity.load_censor_words()`.\n",
    "\n",
    "5. Use the profanity filter to censor the words from the text by applying the `profanity.censor()` function.\n",
    "\n",
    "6. Print both the original text and the filtered text to demonstrate the changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a87417-ab7a-4ffd-821b-30a5119a28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity\n",
    "\n",
    "# Sample text to filter\n",
    "text4 = \"\"\"\n",
    "Hey, you are a really bad person! Stop being so rude.\n",
    "\"\"\"\n",
    "\n",
    "# Define a custom list of words you want to censor\n",
    "custom_bad_words = ['bad', 'rude', 'email', 'person']\n",
    "\n",
    "# Load the custom censor words\n",
    "profanity.load_censor_words(custom_bad_words)\n",
    "\n",
    "# Filter the text\n",
    "filtered_output2 = profanity.censor(text4)\n",
    "\n",
    "print(\"Original Text:\\n\", text4)\n",
    "print(\"\\nFiltered Text:\\n\", filtered_output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c25beb",
   "metadata": {},
   "source": [
    "#### **Task 5: Profanity Censorship with Severity Levels**\n",
    "\n",
    "**Objective:** Implement a function to censor profanities in a text based on severity using `cleantext` and `better_profanity`.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Import Libraries:** Import `clean` from `cleantext` and `profanity` from `better_profanity`.\n",
    "\n",
    "2. **Define Function:** Create `censor_severity(text)`.\n",
    "\n",
    "3. **Set Profanity Lists:** Define `mild_profanities` and `severe_profanities`.\n",
    "\n",
    "4. **Clean Text:** Remove emails and URLs from `text`.\n",
    "\n",
    "5. **Censor Profanities:**\n",
    "   - Replace severe profanities with \"****\".\n",
    "   - Replace mild profanities with a masked version.\n",
    "\n",
    "6. **Return Text:** Return the censored text.\n",
    "\n",
    "7. **Example:** Test with a sample text and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23967c8-1e1f-4104-94f5-3f715914b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "from better_profanity import profanity\n",
    "\n",
    "def censor_severity(text):\n",
    "    # List of milder and stronger profanities (for demonstration)\n",
    "    mild_profanities = [\"damn\", \"crap\"]\n",
    "    severe_profanities = [\"hell\", \"stupid\"]\n",
    "\n",
    "    # Replace emails and URLs first\n",
    "    text = clean(\n",
    "        text,\n",
    "        no_emails=True,\n",
    "        no_urls=True\n",
    "    )\n",
    "\n",
    "    # Handle severe profanity first (complete masking)\n",
    "    for word in severe_profanities:\n",
    "        text = text.replace(word, \"****\")\n",
    "\n",
    "    # Handle mild profanity (partial masking)\n",
    "    for word in mild_profanities:\n",
    "        text = text.replace(word, word[0] + \"**\" + word[-1])\n",
    "\n",
    "    return text\n",
    "\n",
    "text5 = \"This is crap and a damn hell of a situation! Email me at john.doe@example.com\"\n",
    "print(censor_severity(text5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922216bf-e72b-4b66-9bb1-c831f16dfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do not modify this block\n",
    "from test_guardrails import test_guardrails\n",
    "\n",
    "try:\n",
    "    pii_output= detect_pii(text3)\n",
    "except:\n",
    "    pii_output = None\n",
    "\n",
    "try:\n",
    "    censor_output = censor_severity(text5)\n",
    "except:\n",
    "    censor_output = None\n",
    "    \n",
    "try:\n",
    "    test_guardrails.save_answer(toxicity_detector, toxicity_results, filtered_output1, pii_output, filtered_output2, censor_output)\n",
    "except:\n",
    "    print(\"Assign the answers to all the variables properly\")\n",
    "    test_guardrails.remove_pickle()\n",
    "    try:\n",
    "        test_guardrails.save_ans1(toxicity_detector, toxicity_results)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        test_guardrails.save_ans2(filtered_output1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        test_guardrails.save_ans3(pii_output)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        test_guardrails.save_ans4(filtered_output2)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        test_guardrails.save_ans5(censor_output)\n",
    "    except:\n",
    "        pass\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e58b1-6c5f-4d99-8415-8b0c10d8fba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
